{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "from scipy import signal\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from datetime import timedelta\n",
    "#from obspy import UTCDateTime, Catalog, read\n",
    "#from obspy.clients.fdsn import Client\n",
    "#from obspy.taup import TauPyModel\n",
    "\n",
    "\n",
    "from skimage.metrics import structural_similarity as sim\n",
    "\n",
    "from models.N2N_Unet import N2N_Unet_DAS, N2N_Orig_Unet, Cut2Self, U_Net_origi, U_Net, TestNet\n",
    "from metric import Metric\n",
    "from masks import Mask\n",
    "from models.P_Unet import P_U_Net\n",
    "from utils import *\n",
    "from transformations import *\n",
    "from loss import calculate_loss\n",
    "import config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fertig\n"
     ]
    }
   ],
   "source": [
    "def get_test_samples():\n",
    "    \n",
    "    test_path = '../../data/earthquakes/sissle/eq_data_50Hz/test/'\n",
    "    test_paths = sorted([test_path + f for f in os.listdir(test_path)])\n",
    "\n",
    "    indices = [6250,6750,6250,5250,7500,5500]\n",
    "    test_data = []\n",
    "    for i, (p, idx) in enumerate(zip(test_paths, indices)):\n",
    "        with h5py.File(p, 'r') as hf:\n",
    "            test_data.append(hf['DAS'][81:,idx-3000:idx+3000])\n",
    "    test_data = np.stack(test_data)[[2,3,5,0,1,4]]\n",
    "\n",
    "    gutter = 1000\n",
    "    test_data = np.pad(test_data, ((0,0),(0,0),(gutter,gutter)), mode='constant', constant_values=0)\n",
    "    test_data = bandpass(test_data, low=1.0, high=10.0, fs=50, gutter=gutter)\n",
    "    test_scale = test_data.std(axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "    test_data = torch.from_numpy(test_data.copy())\n",
    "    test_scale = torch.from_numpy(test_scale.copy())\n",
    "    \n",
    "    return test_data, test_scale\n",
    "\n",
    "def bandpass(x, low, high, fs, gutter, alpha=0.1):\n",
    "    \"\"\"\n",
    "    alpha: taper length\n",
    "    \"\"\"\n",
    "    \n",
    "    passband = [2 * low/fs, 2 * high/fs]\n",
    "    b, a = signal.butter(2, passband, btype=\"bandpass\")\n",
    "    window = signal.windows.tukey(x.shape[-1], alpha=alpha)\n",
    "    x = signal.filtfilt(b, a, x * window, axis=-1)\n",
    "\n",
    "    return x[..., gutter:-gutter]\n",
    "\n",
    "def generate_synthetic_das(strain_rate, gauge, fs, slowness, nx=512):\n",
    "\n",
    "    # shift\n",
    "    # slowness: 0.0001 s/m = 0.1 s/km   -  0.005 s/m = 5 s/km\n",
    "    # speed: 10,000 m/s = 10 km/s    -  200 m/s = 0.2 km/s\n",
    "    shift = gauge * fs * slowness # L f / v\n",
    "\n",
    "    sample = torch.zeros((nx, len(strain_rate)))\n",
    "    for i in range(nx):\n",
    "        sample[i] = torch.roll(strain_rate, int(i*shift + np.random.randn(1)))\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def shift_traffic_rates(traffic_rates, gauge, fs, slowness):\n",
    "\n",
    "    # shift\n",
    "    # slowness: 0.0001 s/m = 0.1 s/km   -  0.005 s/m = 5 s/km\n",
    "    # speed: 10,000 m/s = 10 km/s    -  200 m/s = 0.2 km/s\n",
    "    shift = gauge * fs * slowness # L f / v\n",
    "    \n",
    "    # traffic_rates shape (512,3000)\n",
    "    traffic_rates = torch.tile(traffic_rates, (1,3))\n",
    "    sample = torch.zeros_like(traffic_rates)\n",
    "    for i in range(len(traffic_rates)):\n",
    "        sample[i] = torch.roll(traffic_rates[i], int(i*shift + np.random.randn(1)))\n",
    "    sample = torch.roll(sample, np.random.randint(0,traffic_rates.shape[1]//3))\n",
    "\n",
    "    return sample\n",
    "\n",
    "class SyntheticTrafficDAS(Dataset):\n",
    "    def __init__(self, eq_strain_rates, traffic_inc, traffic_dec, \n",
    "                 nx=64, nt=256, eq_slowness=(1e-4, 5e-3), log_SNR=(-2,4), traffic_slowness=(3e-2, 6e-2),\n",
    "                 gauge=4, fs=50.0, size=1000):\n",
    "        self.eq_strain_rates = eq_strain_rates / eq_strain_rates.std(dim=-1, keepdim=True)\n",
    "        self.traffic_inc = traffic_inc / traffic_inc.std(dim=-1, keepdim=True)\n",
    "        self.traffic_dec = traffic_dec / traffic_dec.std(dim=-1, keepdim=True)\n",
    "        self.nx = nx\n",
    "        self.nt = nt\n",
    "        self.eq_slowness = eq_slowness\n",
    "        self.log_SNR = log_SNR\n",
    "        self.traffic_slowness = traffic_slowness\n",
    "        self.gauge = gauge\n",
    "        self.fs = fs\n",
    "        self.size = size\n",
    "        \n",
    "        if self.gauge == 20:\n",
    "            self.traffic_inc = self.traffic_inc[:,::5]\n",
    "            self.traffic_dec = self.traffic_dec[:,::5]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample = torch.zeros((self.nx, self.nt))\n",
    "        eq_strain_rate = self.eq_strain_rates[np.random.randint(0,len(self.eq_strain_rates))].clone()\n",
    "        if np.random.random() < 0.5:\n",
    "            eq_strain_rate = torch.flip(eq_strain_rate, dims=(0,))\n",
    "        if np.random.random() < 0.5:\n",
    "            eq_strain_rate *= -1\n",
    "            \n",
    "        slowness = np.random.uniform(*self.eq_slowness)\n",
    "        if np.random.random() < 0.5:\n",
    "            slowness *= -1\n",
    "        eq_das = generate_synthetic_das(eq_strain_rate, self.gauge, self.fs, slowness, nx=self.nx)\n",
    "        idx = np.random.randint(0, 9000-self.nt+1)\n",
    "        eq_das = eq_das[:,idx:idx+self.nt]\n",
    "        \n",
    "        snr = 10 ** np.random.uniform(*self.log_SNR)  # log10-uniform distribution\n",
    "        \n",
    "        eq_das /= eq_das.std(dim=-1, keepdim=True)\n",
    "        amp = 2 * np.sqrt(snr) / torch.abs(eq_das + 1e-10).max()\n",
    "        eq_das *= amp\n",
    "        \n",
    "        if np.random.random() < 0.5:\n",
    "            idx = np.random.randint(0, len(self.traffic_inc))\n",
    "            start = np.random.randint(0, self.traffic_inc.shape[1]-self.nx+1)\n",
    "            traffic_rates = self.traffic_inc[idx, start:start+self.nx].clone()\n",
    "            direction = 1\n",
    "        else:\n",
    "            idx = np.random.randint(0, len(self.traffic_dec))\n",
    "            start = np.random.randint(0, self.traffic_dec.shape[1]-self.nx+1)\n",
    "            traffic_rates = self.traffic_dec[idx, start:start+self.nx].clone()\n",
    "            direction = -1\n",
    "        \n",
    "        slowness = np.random.uniform(*self.traffic_slowness)\n",
    "        traffic_das = shift_traffic_rates(traffic_rates, self.gauge, self.fs, direction*slowness)\n",
    "        \n",
    "        if np.random.random() < 0.5:\n",
    "            if direction == 1:\n",
    "                idx = np.random.randint(0, len(self.traffic_dec))\n",
    "                start = np.random.randint(0, self.traffic_dec.shape[1]-self.nx+1)\n",
    "                traffic_rates = self.traffic_dec[idx, start:start+self.nx].clone()\n",
    "            else:\n",
    "                idx = np.random.randint(0, len(self.traffic_inc))\n",
    "                start = np.random.randint(0, self.traffic_inc.shape[1]-self.nx+1)\n",
    "                traffic_rates = self.traffic_inc[idx, start:start+self.nx].clone()\n",
    "                \n",
    "            slowness = np.random.uniform(*self.traffic_slowness)\n",
    "            traffic_das += (0.3*torch.randn(1).item() + 1) * shift_traffic_rates(traffic_rates, self.gauge, self.fs, -1*direction*slowness)\n",
    "        \n",
    "        \n",
    "        gutter = 100\n",
    "        idx = np.random.randint(gutter, 9000-self.nt-gutter+1)\n",
    "        traffic_das = traffic_das[:,idx-gutter:idx+self.nt+gutter]\n",
    "        traffic_das = torch.from_numpy(bandpass(traffic_das, 1.0, 10.0, self.fs, gutter).copy())\n",
    "        traffic_das /= traffic_das.std(dim=-1, keepdim=True)\n",
    "        \n",
    "        sample = eq_das + traffic_das\n",
    "        scale = sample.std(dim=-1, keepdim=True)\n",
    "        sample /= scale        \n",
    "        return sample.unsqueeze(0), (eq_das / amp).unsqueeze(0), traffic_das.unsqueeze(0), scale.unsqueeze(0), amp\n",
    "    \n",
    "\n",
    "\n",
    "class SyntheticNoiseDAS(Dataset):\n",
    "    def __init__(self, eq_strain_rates, \n",
    "                 nx=11, nt=2048, eq_slowness=(1e-4, 5e-3), log_SNR=(-2,4),\n",
    "                 gauge=4, fs=50.0, size=1000):\n",
    "        self.eq_strain_rates = eq_strain_rates / eq_strain_rates.std(dim=-1, keepdim=True)\n",
    "        self.nx = nx\n",
    "        self.nt = nt\n",
    "        self.eq_slowness = eq_slowness\n",
    "        self.log_SNR = log_SNR\n",
    "        self.gauge = gauge\n",
    "        self.fs = fs\n",
    "        self.size = size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        eq_strain_rate = self.eq_strain_rates[np.random.randint(0,len(self.eq_strain_rates))].clone()\n",
    "        if np.random.random() < 0.5:\n",
    "            eq_strain_rate = torch.flip(eq_strain_rate, dims=(0,))\n",
    "        if np.random.random() < 0.5:\n",
    "            eq_strain_rate *= -1\n",
    "            \n",
    "        slowness = np.random.uniform(*self.eq_slowness)\n",
    "        if np.random.random() < 0.5:\n",
    "            slowness *= -1\n",
    "        eq_das = generate_synthetic_das(eq_strain_rate, self.gauge, self.fs, slowness, nx=self.nx)\n",
    "        j = np.random.randint(0, eq_strain_rate.shape[-1]-self.nt+1)\n",
    "        eq_das = eq_das[:,j:j+self.nt]\n",
    "        \"\"\"\n",
    "        snr = 10 ** np.random.uniform(*self.log_SNR)  # log10-uniform distribution\n",
    "        amp = 2 * np.sqrt(snr) / torch.abs(eq_das + 1e-10).max()\n",
    "        eq_das *= amp\n",
    "\n",
    "        # 1-10 Hz filtered Gaussian white noise\n",
    "        gutter = 100\n",
    "        noise = np.random.randn(self.nx, self.nt + 2*gutter)\n",
    "        noise = torch.from_numpy(bandpass(noise, 1.0, 10.0, self.fs, gutter).copy())\n",
    "\n",
    "        sample = eq_das + noise\n",
    "        scale = sample.std(dim=-1, keepdim=True)\n",
    "        sample /= scale\n",
    "        #           nois_image,         clean               noise         std der Daten (b,c,nx,1)  Ampllitude vom DAS (b)\n",
    "        return sample.unsqueeze(0), eq_das.unsqueeze(0), noise.unsqueeze(0), scale.unsqueeze(0), amp\n",
    "        \"\"\"\n",
    "        return eq_das.unsqueeze(0)\n",
    "\n",
    "\n",
    "class RealDAS(Dataset):\n",
    "\n",
    "    def __init__(self, data, nx=128, nt=512, size=1000):\n",
    "        \n",
    "        self.data = torch.from_numpy(data.copy())\n",
    "        self.nx, self.nt = nx, nt\n",
    "        self.size = size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        n, nx_total, nt_total = self.data.shape\n",
    "        nx = np.random.randint(0, nx_total - self.nx)\n",
    "        nt = np.random.randint(0, nt_total - self.nt)\n",
    "        \n",
    "        patch = self.data[idx % n, nx:nx+self.nx, nt:nt+self.nt].clone()\n",
    "        \n",
    "        if np.random.random() < 0.5:\n",
    "            patch = torch.flip(patch, dims=(0,))\n",
    "        if np.random.random() < 0.5:\n",
    "            patch = torch.flip(patch, dims=(1,))\n",
    "        if np.random.random() < 0.5:\n",
    "            patch *= -1 \n",
    "\n",
    "        return patch.unsqueeze(0)\n",
    "print(\"fertig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateSigma(noise_image, vector):\n",
    "    sigmas = torch.linspace(0.1, 0.7, 61)\n",
    "    quality_metric = []\n",
    "    for sigma in sigmas:\n",
    "        \n",
    "        simple_out = noise_image + sigma**2 * vector.detach()\n",
    "        simple_out = (simple_out + 1) / 2\n",
    "        quality_metric += [Metric.tv_norm(simple_out).item()]\n",
    "    \n",
    "    sigmas = sigmas.numpy()\n",
    "    quality_metric = np.array(quality_metric)\n",
    "    best_idx = np.argmin(quality_metric)\n",
    "    return quality_metric[best_idx], sigmas[best_idx]\n",
    "\n",
    "\n",
    "# Beispielhafte Funktion für `generate_synthetic_das`\n",
    "def generate_synthetic_das(eq_strain_rate, gauge, fs, slowness, nx):\n",
    "    # Dummy-Implementierung (bitte durch Ihre eigene Logik ersetzen)\n",
    "    return eq_strain_rate.unsqueeze(0).repeat(nx, 1)\n",
    "\n",
    "# Beispielhafte Funktion für `bandpass`\n",
    "def bandpass(data, lowcut, highcut, fs, gutter):\n",
    "    # Dummy-Implementierung (bitte durch Ihre eigene Logik ersetzen)\n",
    "    return data[:, gutter:-gutter]\n",
    "print(\"fertig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LaAlexPi\\AppData\\Local\\Temp\\ipykernel_16620\\2902701896.py:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  sample[i] = torch.roll(strain_rate, int(i*shift + np.random.randn(1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "eq_strain_rates = torch.randn(10, 5000)  # Beispielhafte Daten, bitte durch Ihre eigenen Daten ersetzen\n",
    "\n",
    "#eq_strain_rates = np.load(\"data/DAS/SIS-rotated_train_50Hz.npy\")\n",
    "#eq_strain_rates = torch.tensor(eq_strain_rates)\n",
    "\n",
    "dataset = SyntheticNoiseDAS(eq_strain_rates)\n",
    "\n",
    "# Erstellen des DataLoaders\n",
    "dataLoader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "#batch_idx, original = next(enumerate(dataLoader))\n",
    "for batch_idx, original in enumerate((dataLoader)):#tqdm\n",
    "    print(batch_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5000])\n",
      "(1444, 9000)\n",
      "torch.Size([1444, 9000])\n",
      "tensor([[1.0004e-08],\n",
      "        [1.2356e-08],\n",
      "        [7.5059e-09],\n",
      "        ...,\n",
      "        [1.1134e-08],\n",
      "        [1.5376e-08],\n",
      "        [1.4400e-08]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "test = torch.randn(10, 5000)\n",
    "print(test.shape)\n",
    "print(eq_strain_rates.shape)\n",
    "eq_strain_rates_torch = torch.tensor(eq_strain_rates)\n",
    "print(eq_strain_rates_torch.shape)\n",
    "print(eq_strain_rates_torch.std(dim=-1,keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n2n_create_2_input(device, methode, original, noise_images):\n",
    "    if \"2_input\" in methode:\n",
    "        if config.useSigma:\n",
    "            noise_image2 = add_norm_noise(original, config.sigma, -1,-1,False)\n",
    "        else:\n",
    "            noise_image2, alpha = add_noise_snr(original, snr_db=config.sigmadb)\n",
    "    else:\n",
    "        if config.useSigma:\n",
    "            noise_image2 = add_norm_noise(noise_images, config.methodes['n2noise_1_input']['secoundSigma'], -1,-1,False)\n",
    "        else:\n",
    "            noise_image2, alpha = add_noise_snr(noise_images, snr_db=config.methodes['n2noise_1_input']['secoundSigma']) \n",
    "    noise_image2 = noise_image2.to(device)\n",
    "    return noise_image2#original, noise_images  are onlly if n2void"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n2noise_2_input\n",
      "250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LaAlexPi\\AppData\\Local\\Temp\\ipykernel_16620\\2902701896.py:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  sample[i] = torch.roll(strain_rate, int(i*shift + np.random.randn(1)))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_NOT_INITIALIZED",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 135\u001b[0m\n\u001b[0;32m    133\u001b[0m     loss, denoised, original, noise_images, optional_tuples \u001b[38;5;241m=\u001b[39m calculate_loss(model, device, dataLoader, methode, true_noise_sigma, batch_idx, original, noise_images, noise_images2, augmentation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dropout_rate\u001b[38;5;241m=\u001b[39mdropout_rate, sigma_info\u001b[38;5;241m=\u001b[39msigma_info)\n\u001b[0;32m    134\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 135\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m#if config.methodes[methode]['sheduler']:\u001b[39;00m\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;66;03m#scheduler.step()\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m#log Data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python\\3_11_0\\virtualenviroment\\noise2x\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\3_11_0\\virtualenviroment\\noise2x\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\3_11_0\\virtualenviroment\\noise2x\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED"
     ]
    }
   ],
   "source": [
    "eq_strain_rates = torch.randn(10, 5000)  # Beispielhafte Daten, bitte durch Ihre eigenen Daten ersetzen\n",
    "\n",
    "#eq_strain_rates = np.load(\"data/DAS/SIS-rotated_train_50Hz.npy\")\n",
    "\n",
    "dataset = SyntheticNoiseDAS(eq_strain_rates)\n",
    "\n",
    "# Erstellen des DataLoaders\n",
    "dataLoader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "methoden = [\"n2noise\", \"n2score\", \"n2self\", \"n2self j-invariant\", \"n2same\", \"n2same batch\", \"n2info\", \"self2self\", \"n2void\"]\n",
    "#methoden = [\"n2void\"]\n",
    "methoden = list(config.methodes.keys())\n",
    "sigma = 2\n",
    "mode = \"train\"\n",
    "lambda_inv=2\n",
    "dropout_rate=0\n",
    "max_Predictions=100\n",
    "sigma_info=1\n",
    "\n",
    "for methode in methoden:\n",
    "    print(methode)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    #model = U_Net(in_chanel=1, scaling_kernel_size=(1,2), batchNorm=True).to(device)\n",
    "    model = TestNet(1,1).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "    # Iterieren durch den DataLoader\n",
    "    batch_idx = 0\n",
    "    print(len(dataLoader))\n",
    "    \"\"\"\n",
    "    for batch in dataLoader:\n",
    "        samples, eq_das, noise, scales, amps = batch\n",
    "        samples = samples.to(device).float()\n",
    "        eq_das = eq_das.to(device).float()\n",
    "        \n",
    "        loss, denoised, original, noise_images, optional_tuples = calculate_loss(model, device, dataloader, methode, 2, noise.std(), batch_idx, eq_das, samples, augmentation=True, lambda_inv=2, dropout_rate=0.3, samples=10, num_patches_per_img=None, num_masked_pixels=8, sigma_info=1)\n",
    "        batch_idx += 1\n",
    "        if batch_idx%100 == 0:\n",
    "            print(loss)\n",
    "    \"\"\"\n",
    "    all_marked = 0\n",
    "    for batch_idx, original in enumerate((dataLoader)):#tqdm\n",
    "        original = original.to(device)\n",
    "        batch = original.shape[0]\n",
    "        if config.useSigma:\n",
    "            noise_images = add_norm_noise(original, sigma, a=-1, b=1, norm=False)\n",
    "            true_noise_sigma = sigma\n",
    "        else:\n",
    "            noise_images, true_noise_sigma = add_noise_snr(original, snr_db=sigma)\n",
    "        \n",
    "        noise_images = noise_images.to(device)\n",
    "        if \"n2noise\" in methode:\n",
    "            noise_images2 = n2n_create_2_input(device, methode, original, noise_images)\n",
    "            if \"test\" in methode:\n",
    "                noise_images2 = add_norm_noise(original, config.methodes['n2noise_2_input_test']['secoundSigma'], a=-1, b=1, norm=False)\n",
    "            #noise_images2 = torch.clip(noise_images2, 0,1.0)\n",
    "        else:\n",
    "            noise_images2 = None\n",
    "        #noise_images = torch.clip(noise_images, 0,1.0)\n",
    "\n",
    "        if mode==\"test\" or mode ==\"validate\":\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                loss, _, _, _, optional_tuples = calculate_loss(model, device, dataLoader, methode, true_noise_sigma, batch_idx, original, noise_images, noise_images2, augmentation=False, dropout_rate=dropout_rate)\n",
    "                (_, _, _, est_sigma_opt) = optional_tuples\n",
    "                if \"n2noise\" in methode:\n",
    "                    denoised = model (noise_images)            \n",
    "                elif \"n2score\"  in methode:\n",
    "                    #true_sigma_score.append(true_noise_sigma)\n",
    "                    vector =  model(noise_images)\n",
    "                    best_tv, best_sigma = evaluateSigma(noise_images, vector)\n",
    "                    #best_sigmas.append(best_sigma)\n",
    "                    #all_tvs.append(best_tv)\n",
    "                    denoised = noise_images + best_sigma**2 * vector\n",
    "                elif \"n2self\" in methode:\n",
    "                    #if \"j-invariant\" in methode:\n",
    "                    if \"j-invariant\" in config.methodes[methode]['erweiterung']:\n",
    "                        denoised = Mask.n2self_jinv_recon(noise_images, model)\n",
    "                    else:\n",
    "                        denoised = model(noise_images)\n",
    "                elif \"n2same\" in methode:\n",
    "                    denoised = model(noise_images)\n",
    "                elif \"n2void\" in methode:\n",
    "                    #calculate mean and std for each Image in batch in every chanal\n",
    "                    #mean = noise_images.mean(dim=[0,2,3])\n",
    "                    #std = noise_images.std(dim=[0,2,3])\n",
    "                    #noise_images = (noise_images - mean[None, :, None, None]) / std[None, :, None, None]\n",
    "                    noise_images = (noise_images - noise_images.mean(dim=(1, 2), keepdim=True)) / noise_images.std(dim=(1, 2), keepdim=True)\n",
    "                    denoised = model(noise_images)\n",
    "                elif \"s2self\" in methode:\n",
    "                    denoised = torch.ones_like(noise_images)\n",
    "                    for i in range(max_Predictions):\n",
    "                        _, denoised_tmp, _, _, flip = calculate_loss(model, device, dataLoader, methode, true_noise_sigma, batch_idx, original, noise_images, noise_images2, augmentation, dropout_rate=dropout_rate)\n",
    "                        (lr, ud, _, _) = flip\n",
    "                        #denoised_tmp = filp_lr_ud(denoised_tmp, lr, ud)\n",
    "                        denoised = denoised + denoised_tmp\n",
    "                    denoised = denoised / max_Predictions\n",
    "                    denoised = (denoised+1)/2\n",
    "                elif \"n2info\" in methode:\n",
    "                    #TODO: normalisierung ist in der implementation da, aber ich habe es noch nicht im training gefunden\n",
    "                    if mode==\"test\":\n",
    "                        denoised = model(noise_images)\n",
    "                    else:\n",
    "                        #loss, denoised, loss_rec, loss_inv, marked_pixel = n2same(noise_images, device, model, lambda_inv)\n",
    "                        loss, denoised, loss_rec, loss_inv, marked_pixel = n2info(noise_images, model, device, sigma_info)\n",
    "                        all_marked += marked_pixel\n",
    "                        lex += loss_rec\n",
    "                        lin += loss_inv\n",
    "                        n_partition = (denoised-noise_images).view(denoised.shape[0], -1) # (b, c*w*h)\n",
    "                        n_partition = torch.sort(n_partition, dim=1).values #descending=False\n",
    "                        n = torch.cat((n, n_partition), dim=0)\n",
    "                        if batch_idx == len(dataLoader)-1:\n",
    "                            e_l = 0\n",
    "                            for i in range(config.methodes['n2info']['predictions']): #kmc\n",
    "                                #to big for torch.multinomial if all pictures from validation should be used\n",
    "                                #samples = torch.tensor(torch.multinomial(n.view(-1), n.shape[1], replacement=True))#.view(1, n.shape[1])\n",
    "                                #samples = torch.sort(samples).values\n",
    "                                samples = np.sort(np.random.choice((n.cpu()).reshape(-1),[1, n.shape[1]])) #(1,49152)\n",
    "                                e_l += torch.mean((n-torch.from_numpy(samples).to(device))**2)\n",
    "                            lex = lex / (len(dataLoader) * denoised.shape[0])\n",
    "                            lin = lin / all_marked\n",
    "                            e_l = e_l / config.methodes['n2info']['predictions']\n",
    "                            #estimated_sigma = (lin)**0.5 + (lin + lex-e_l)**0.5 #inplementation from original github of noise2info\n",
    "                            m = len(dataLoader) * denoised.shape[0] *3*128*128 #TODO: is m right?\n",
    "                            estimated_sigma = lex + (lex**2 * m *(lin-e_l))**0.5/m #from paper\n",
    "                            print('new sigma_loss is ', estimated_sigma)\n",
    "                            if 0 < estimated_sigma < sigma_n:\n",
    "                                sigma_n = float(estimated_sigma)\n",
    "                                print('sigma_loss updated to ', estimated_sigma)\n",
    "                        \n",
    "        else:\n",
    "            model.train()\n",
    "            #original, noise_images are only important if n2void\n",
    "            loss, denoised, original, noise_images, optional_tuples = calculate_loss(model, device, dataLoader, methode, true_noise_sigma, batch_idx, original, noise_images, noise_images2, augmentation=False, dropout_rate=dropout_rate, sigma_info=sigma_info)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #if config.methodes[methode]['sheduler']:\n",
    "                #scheduler.step()\n",
    "\n",
    "        #log Data\n",
    "        denoised = (denoised-denoised.min())  / (denoised.max() - denoised.min())\n",
    "        \n",
    "        original = (original-original.min())  / (original.max() - original.min())\n",
    "        psnr_batch = Metric.calculate_psnr(original, denoised)\n",
    "        #similarity_batch, diff_picture = Metric.calculate_similarity(original, denoised)\n",
    "        \n",
    "        if batch_idx%100 == 0:\n",
    "            print(loss)\n",
    "            print(psnr_batch)\n",
    "            print()\n",
    "            \n",
    "        \"\"\"\n",
    "        DAS-syntttetische  Datten\n",
    "\n",
    "        noise -> NaN\n",
    "        score -> 0,99  27.4\n",
    "        self ->  0,05  26,19\n",
    "        self_j -> 0,05  25,69\n",
    "        same -> 0,53  27,76\n",
    "        same_b -> 0,52  27,44\n",
    "        info -> 0,64  27,79\n",
    "        self2self -> Probleme\n",
    "        n2void -> Probbleme\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noise2x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
