{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity as sim\n",
    "\n",
    "from models.N2N_Unet import N2N_Unet_DAS, N2N_Orig_Unet, Cut2Self\n",
    "from metric import Metric\n",
    "from masks import Mask\n",
    "from utils import *\n",
    "from transformations import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#sys.path.insert(0, str(Path(__file__).resolve().parents[3])) #damit die Pfade auf dem Server richtig waren (copy past von PG)\n",
    "from absl import app\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patches_from_list(data, num_patches_per_img=None, shape=(64, 64), augment=True, shuffle=False):\n",
    "        \"\"\"\n",
    "        Extracts patches from 'list_data', which is a list of images, and returns them in a 'numpy-array'. The images\n",
    "        can have different dimensionality.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data                : list(array(float))\n",
    "                            List of images with dimensions 'SZYXC' or 'SYXC'\n",
    "        num_patches_per_img : int, optional(default=None)\n",
    "                            If 'None', as many patches as fit i nto the dimensions are extracted.\n",
    "                            Else may generate overlapping patches\n",
    "        shape               : tuple(int), optional(default=(256, 256))\n",
    "                            Shape of the extracted patches.\n",
    "        augment             : bool, optional(default=True)\n",
    "                            Rotate the patches in XY-Plane and flip them along X-Axis. This only works if the patches are square in XY.\n",
    "        shuffle             : bool, optional(default=False)\n",
    "                            Shuffles extracted patches across all given images (data).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        patches : array(float)\n",
    "                Numpy-Array with the patches. The dimensions are 'SZYXC' or 'SYXC'\n",
    "        \"\"\"\n",
    "        patches = []\n",
    "        for img in data:\n",
    "            patches.append( generate_patches(img.unsqueeze(0), num_patches=num_patches_per_img, shape=shape, augment=augment) )\n",
    "        patches = torch.cat(patches, dim=0)\n",
    "\n",
    "        if shuffle:\n",
    "            indices = torch.randperm(len(patches))\n",
    "            patches = patches[indices]\n",
    "        return patches\n",
    "\n",
    "def generate_patches(data, num_patches=None, shape=(64, 64), augment=True):\n",
    "    \"\"\"\n",
    "    Extracts patches from 'data'. The patches can be augmented, which means they get rotated three times\n",
    "    in XY-Plane and flipped along the X-Axis. Augmentation leads to an eight-fold increase in training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data        : list(array(float))\n",
    "                List of images with dimensions 'SZYXC' or 'SYXC'\n",
    "    num_patches : int, optional(default=None)\n",
    "                Number of patches to extract per image. If 'None', as many patches as fit into the\n",
    "                dimensions are extracted.\n",
    "    shape       : tuple(int), optional(default=(256, 256))\n",
    "                Shape of the extracted patches.\n",
    "    augment     : bool, optional(default=True)\n",
    "                Rotate the patches in XY-Plane and flip them along X-Axis. This only works if the patches are square in XY.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    patches : array(float)\n",
    "            Numpy-Array containing all patches (randomly shuffled along S-dimension).\n",
    "            The dimensions are 'SZYXC' or 'SYXC'\n",
    "    \"\"\"\n",
    "    patches = __extract_patches__(data, num_patches=num_patches, shape=shape)\n",
    "    if augment and shape[0] == shape[1]:\n",
    "        patches = __augment_patches__(patches)\n",
    "\n",
    "    if num_patches is not None:\n",
    "        indices = torch.randint(len(patches), (num_patches,))\n",
    "        patches = patches[indices]\n",
    "\n",
    "    print('Generated patches:', patches.shape)\n",
    "    return patches\n",
    "    \n",
    "def __extract_patches__(data, num_patches=None, shape=(64, 64)):\n",
    "    patches = []\n",
    "    if num_patches is None:\n",
    "        if data.shape[-2] >= shape[0] and data.shape[-1] >= shape[1]:\n",
    "            for y in range(0, data.shape[-2] - shape[0] + 1, shape[0]):\n",
    "                for x in range(0, data.shape[-1] - shape[1] + 1, shape[1]):\n",
    "                    patches.append(data[..., y:y + shape[0], x:x + shape[1]])\n",
    "    else:\n",
    "        for i in range(num_patches):\n",
    "            y, x = torch.randint(0, data.shape[-2] - shape[0] + 1, (2,))\n",
    "            patches.append(data[..., y:y + shape[0], x:x + shape[1]])\n",
    "    return torch.cat(patches, axis=0)\n",
    "\n",
    "def __augment_patches__(patches):\n",
    "    augmented = torch.cat((patches,\n",
    "                            torch.rot90(patches, 1, (-2, -1)),\n",
    "                            torch.rot90(patches, 2, (-2, -1)),\n",
    "                            torch.rot90(patches, 3, (-2, -1))),\n",
    "                            dim=0)\n",
    "    augmented = torch.cat((augmented, torch.flip(augmented, [-2])))\n",
    "    return augmented\n",
    "\n",
    "def show_tensor_as_picture(img):\n",
    "    img = img.cpu().detach()\n",
    "    if len(img.shape)==4:\n",
    "        img = img[0]\n",
    "    if img.shape[0] == 3:\n",
    "        img = img.permute(1,2,0)\n",
    "    plt.imshow(img, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "def n2void_mask(image_shape, num_masked_pixels=8):\n",
    "    \"\"\"\n",
    "    uniform_pixel_selection_mask\n",
    "    Erstellt eine Uniform Pixel Selection Maske.\n",
    "    \n",
    "    image_shape (tuple): Die Form des Bildes (batch, channels, height, width).\n",
    "    num_masked_pixels (int): Die Anzahl der maskierten Pixel, die ausgewählt werden sollen.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(image_shape)==3:\n",
    "        return select_random_pixels(image_shape, num_masked_pixels)\n",
    "    else:\n",
    "        mask_for_batch = []\n",
    "        for i in range(image_shape[0]):\n",
    "            mask_for_batch.append(select_random_pixels((image_shape[1],image_shape[2],image_shape[3]), num_masked_pixels))\n",
    "        return torch.stack(mask_for_batch)\n",
    "        \n",
    "\n",
    "    \n",
    "def select_random_pixels(image_shape, num_masked_pixels):\n",
    "    num_pixels = image_shape[1] * image_shape[2]\n",
    "    # Erzeuge zufällige Indizes für die ausgewählten maskierten Pixel\n",
    "    masked_indices = torch.randperm(num_pixels)[:num_masked_pixels]\n",
    "    mask = torch.zeros(image_shape[1], image_shape[2])\n",
    "    # Piel in Maske auf 1 setzen\n",
    "    mask.view(-1)[masked_indices] = 1\n",
    "    # Mache für alle Chanels\n",
    "    mask = mask.unsqueeze(0).expand(image_shape[0], -1, -1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((128, 128)), #TODO:crop und dann resize\n",
    "    transforms.RandomResizedCrop((128,128)),\n",
    "    transforms.ToTensor(),                  # PIL-Bild in Tensor\n",
    "    transforms.Lambda(lambda x: x.float()),  # in Float\n",
    "    transforms.Lambda(lambda x: x / torch.max(x)) #skallieren auf [0,1]\n",
    "])\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "methode=\"n2n_orig\"\n",
    "#store_path = log_files()\n",
    "#run = wandb.init(entity=\"\", project=\"my-project-name\", anonymous=\"allow\")\n",
    "#writer = SummaryWriter(log_dir=os.path.join(store_path, \"tensorboard\"))\n",
    "\n",
    "celeba_dir = 'dataset/celeba_dataset'\n",
    "dataset = datasets.CelebA(root=celeba_dir, split='train', download=True, transform=transform)\n",
    "dataset_validate = datasets.CelebA(root=celeba_dir, split='valid', download=True, transform=transform)\n",
    "dataLoader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "dataLoader_validate = DataLoader(dataset_validate, batch_size=64, shuffle=True)\n",
    "\n",
    "mask = Mask.cut2self_mask((128,128), 64).to(device)\n",
    "\n",
    "model = N2N_Orig_Unet(3,3).to(device)\n",
    "model = \n",
    "#model = Cut2Self(mask).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original, label = next(iter(dataLoader))\n",
    "    \n",
    "print(original.shape)\n",
    "print(label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_image = (original + torch.randn_like(original) * (0.5)).to(device)\n",
    "patches = generate_patches_from_list(noise_image, num_patches_per_img=6)\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "show_tensor_as_picture(patches[0])\n",
    "show_tensor_as_picture(patches[1])\n",
    "show_tensor_as_picture(patches[2])\n",
    "show_tensor_as_picture(patches[3])\n",
    "show_tensor_as_picture(patches[4])\n",
    "show_tensor_as_picture(patches[5])\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "mask = n2void_mask(patches.shape, num_masked_pixels=8).to(device)\n",
    "print(patches.shape)\n",
    "print(noise_image.shape)\n",
    "print(mask.shape)\n",
    "print((1-mask).shape)\n",
    "\n",
    "mask_noise = patches * (1-mask)\n",
    "#show_tensor_as_picture(mask_noise) #TODO: ist maske richtig? -> nicht schwärzen sondern, austauschen durch anderes Pixxeln in window-size\n",
    "\n",
    "\n",
    "\n",
    "cords = torch.nonzero(mask)\n",
    "for pixel_idx in range(8):\n",
    "    x, y = cords[pixel_idx]\n",
    "    new_x = max(0, min(patches.shape[2] - 1, x + torch.randint(-p//2, p//2 + 1, (1,)).item()))\n",
    "    new_y = max(0, min(patches.shape[2] - 1, y + torch.randint(-p//2, p//2 + 1, (1,)).item()))\n",
    "    patches[:, y, x] = patches[:, new_y, new_x]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "denoised = model(mask_noise)\n",
    "denoised_pixel = denoised * mask\n",
    "target_pixel = patches * mask\n",
    "\n",
    "show_tensor_as_picture(mask_noise)\n",
    "show_tensor_as_picture(denoised)\n",
    "show_tensor_as_picture(denoised_pixel)\n",
    "show_tensor_as_picture(target_pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = n2void_mask(patches.shape, num_masked_pixels=8).to(device)\n",
    "print(\"maks.shape: \", mask.shape)\n",
    "print(\"patches.shape: \", patches.shape)\n",
    "print(\"--------\")\n",
    "p=5\n",
    "cords = torch.nonzero(mask)\n",
    "bearbeittete_Bilder = patches.clone()\n",
    "num_masked_pixels=8\n",
    "memory = []\n",
    "for pixel_idx in range(cords.shape[0]):\n",
    "    if pixel_idx > 16:\n",
    "        break\n",
    "    batch, chanel, x, y = cords[pixel_idx]\n",
    "    batch, chanel, x, y = batch.item(), chanel.item(), x.item(), y.item()\n",
    "    print(\"batch: \", batch)\n",
    "    print(\"chanel: \", chanel)\n",
    "    print(\"x,y: \", x,y)\n",
    "    if chanel != 0:\n",
    "        bearbeittete_Bilder[batch, chanel, x, y] = patches[batch, chanel, memory[pixel_idx%num_masked_pixels][0], memory[pixel_idx%num_masked_pixels][1]]\n",
    "        if chanel==3 and (pixel_idx%num_masked_pixels)==(num_masked_pixels-1):\n",
    "            memory = []\n",
    "    else: \n",
    "        new_x = max(0, min(bearbeittete_Bilder.shape[2] - 1, x + torch.randint(-p//2, p//2 + 1, (1,)).item()))\n",
    "        new_y = max(0, min(bearbeittete_Bilder.shape[2] - 1, y + torch.randint(-p//2, p//2 + 1, (1,)).item()))\n",
    "        memory.append((new_x, new_y))\n",
    "        bearbeittete_Bilder[batch, chanel, x, y] = patches[batch, chanel, new_x, new_y]\n",
    "\n",
    "print(\"new x: \", new_x)\n",
    "print(\"new y: \", new_y)\n",
    "print(\"bearbeitete Bilder.shape: \", bearbeittete_Bilder.shape)\n",
    "show_tensor_as_picture(patches[0]*(mask[0]).to(device))\n",
    "show_tensor_as_picture(bearbeittete_Bilder[0]*(mask[0]).to(device))\n",
    "show_tensor_as_picture(patches[2]*(mask[2]).to(device))\n",
    "show_tensor_as_picture(bearbeittete_Bilder[2]*(mask[2]).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_tensor_as_picture(patches*(1-mask).to(device))\n",
    "show_tensor_as_picture(bearbeittete_Bilder.to(device))\n",
    "\n",
    "show_tensor_as_picture(patches[2]*(mask[2]).to(device))\n",
    "show_tensor_as_picture(bearbeittete_Bilder[2]*(mask[2]).to(device))\n",
    "print(torch.count_nonzero(mask[0]).item())\n",
    "\n",
    "print(mask[0][0])\n",
    "print(mask[0][1])\n",
    "print(mask[0][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original, label = next(iter(dataLoader))\n",
    "original = original.to(device)\n",
    "noise1 = (torch.randn_like(original)).to(device)\n",
    "noise2 = (torch.randn_like(original) * 0.5).to(device)\n",
    "\n",
    "blue_chanel = torch.zeros_like(noise1).to(device)\n",
    "blue_chanel[:,2,:,:] = 1\n",
    "\n",
    "red_chanel = torch.zeros_like(noise1).to(device)\n",
    "red_chanel[:,0,:,:] = 1\n",
    "\n",
    "blue_m = noise1*red_chanel\n",
    "red_m = noise2*blue_chanel\n",
    "\n",
    "show_tensor_as_picture(original+blue_m)\n",
    "show_tensor_as_picture(original+blue_m+red_m)\n",
    "show_tensor_as_picture(blue_m)\n",
    "show_tensor_as_picture(red_m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CelebA' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m celeba_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/celeba_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     36\u001b[0m dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mCelebA(root\u001b[38;5;241m=\u001b[39mceleba_dir, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m())\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#dataLoader = DataLoader(dataset, batch_size=64, shuffle=True)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CelebA' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CelebA' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#dataset = datasets.CelebA(root=celeba_dir, split='train', download=True, transform=transform)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CelebA' object has no attribute 'data'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denoise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
